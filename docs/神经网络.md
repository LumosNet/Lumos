### 神经网络的基本流程

前向传播、反向传播、权重更新

- 前向传播

  顺序计算每一层输出阈值，最后计算出模型损失

- 反向传播

  反向传播模型损失，计算权重梯度

- 权重更新

  利用梯度下降法更新权重

任何神经网络模型都遵守该基本流程



### DNN

深度全连接神经网络的基本过程是最清晰的。由于该模型可以更好的划分为线性模型的复合，所以其推到流程十分简单。

#### 模型抽象

设有一个DNN模型有以下参数设置

- 损失函数L
- 隐层激活函数σ
- 隐层权重w
- 隐层偏移量b
- 梯度δ

我们做如下声明

隐层计算输出（没有激活）z，隐层计算输出（通过激活）a

隐层计算如下：
$$
\begin{aligned}
&z_{l}=w_{l}a_{l-1}+b_{l} \\
&a_{l}=\sigma(z_{l})=\sigma(w_{l}a_{l-1}+b_{l})
\end{aligned}
$$
设隐层计算为一个函数g，则整个DNN模型可表示为函数f
$$
f=L(g_{l}(g_{l-1}(...g(x))))
$$
可以看出整个DNN模型是一个线性函数的复合



#### 反向推导

由于我们利用梯度下降法来更新模型权重，所以我们需要求每一个权重关于损失的梯度

求梯度，我们就要求损失关于权重的偏导（梯度：全部偏导数组成的向量）
$$
\frac{\partial L}{\partial w}\quad \frac{\partial L}{\partial b}
$$
由复合函数求导的链式法则，有如下过程：

设求第k层权重w，b的梯度
$$
\begin{aligned}
&D_{w_{k}}=\frac{\partial L}{\partial z_{L}} \dot\ \frac{\partial z_{L}}{\partial z_{L-1}} \dot\
\frac{\partial z_{L-1}}{\partial z_{L-2}} \cdots\ \frac{\partial z_{k+1}}{\partial z_{k}} \dot\ \frac{\partial z_{k}}{\partial w_{k}} \\
&D_{b_{k}}=\frac{\partial L}{\partial z_{L}} \dot\ \frac{\partial z_{L}}{\partial z_{L-1}} \dot\
\frac{\partial z_{L-1}}{\partial z_{L-2}} \cdots\ \frac{\partial z_{k+1}}{\partial z_{k}} \dot\ \frac{\partial z_{k}}{\partial b_{k}}
\end{aligned}
$$
可以看出这是一个可递归过程，设输出层为第l（小写L）层，则输出层权重梯度为
$$
\begin{aligned}
&D_{w_{l}}=\frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial w_{l}} \\
&D_{b_{l}}=\frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial b_{l}}
\end{aligned}
$$
第l-1层权重梯度为
$$
\begin{aligned}
&D_{w_{l-1}}=\frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial z_{l-1}} \dot\ \frac{\partial z_{l-1}}{\partial w_{l-1}} \\
&D_{b_{l-1}}=\frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial z_{l-1}} \dot\ \frac{\partial z_{l-1}}{\partial b_{l-1}}
\end{aligned}
$$
第l-2层权重梯度为
$$
\begin{aligned}
&D_{w_{l-2}}=\frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial z_{l-1}} \dot\ \frac{\partial z_{l-1}}{\partial z_{l-2}} \dot\ \frac{\partial z_{l-2}}{\partial w_{l-2}} \\
&D_{b_{l-2}}=\frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial z_{l-1}} \dot\ \frac{\partial z_{l-1}}{\partial z_{l-2}} \dot\
\frac{\partial z_{l-2}}{\partial b_{l-2}}
\end{aligned}
$$
我们可以提取出公式中递归部分
$$
\frac{\partial L}{\partial z_{l}} \quad \frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial z_{l-1}} \quad \frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial z_{l-1}} \dot\ \frac{\partial z_{l-1}}{\partial z_{l-2}}
$$
则设
$$
\begin{aligned}
\delta_{k}&=\frac{\partial L}{\partial z_{L}} \dot\ \frac{\partial z_{L}}{\partial z_{L-1}} \dot\
\frac{\partial z_{L-1}}{\partial z_{L-2}} \cdots\ \frac{\partial z_{k+1}}{\partial z_{k}} \\
&=\delta_{k+1} \dot\ \frac{\partial z_{k+1}}{\partial z_{k}}
\end{aligned}
$$
所以对于每一层只需要计算
$$
\frac{\partial z_{k+1}}{\partial z_{k}} \quad \frac{\partial z_{k}}{\partial w_{k}} \quad \frac{\partial z_{k}}{\partial b_{k}}
$$
则逐层递归就可以求出每一层权重梯度了，再通过梯度下降公式更新权重



#### 计算

我们现在来计算
$$
\frac{\partial z_{k+1}}{\partial z_{k}} \quad \frac{\partial z_{k}}{\partial w_{k}} \quad \frac{\partial z_{k}}{\partial b_{k}}
$$
由隐层计算公式可知
$$
\begin{aligned}
\frac{\partial z_{k+1}}{\partial z_{k}}&=
\frac{\partial (w_{k+1}a_{k}+b_{k+1})}{\partial z_{k}} \\
&=\frac{\partial (w_{k+1} \sigma(z_{k+1})+b_{k+1})}{\partial z_{k}} \\
&=(w_{k+1})^{T} \odot \sigma^{'}(z_{l})
\end{aligned}
$$
则
$$
\begin{aligned}
\delta_{k}&=(w_{k+1})^{T} \delta_{k+1} \odot \sigma^{'}(z_{l})
\end{aligned}
$$
再计算权重的偏导
$$
\begin{aligned}
&\frac{\partial z_{k}}{\partial w_{k}}
=\frac{\partial (w_{k}a_{k-1}+b_{k})}{\partial w_{k}}=(a_{k-1})^{T} \\
&\frac{\partial z_{k}}{\partial b_{k}}
=\frac{\partial (w_{k}a_{k-1}+b_{k})}{\partial b_{k}}=1
\end{aligned}
$$
所以综上所述
$$
\begin{aligned}
&D_{w_{k}}=\delta_{k} (a_{k-1})^{T} \\
&D_{b_{k}}=\delta_{k}
\end{aligned}
$$


### CNN

卷积神经网络的基本结构由卷积、池化、全连接组成，虽然现在还有残差、直连、批次归一化等操作，但是基本结构是不变的

#### 卷积Convolution

卷积神经网络最基本操作就是卷积，如下图，卷积核以滑动窗口的模式，分别与对应元素相乘并求和

![](.\image\卷积.png)
$$
\begin{aligned}
y_{11}=x_{11}w_{11}+x_{12}w_{12}+x_{13}w_{13}+x_{21}w_{21}+x_{22}&w_{22}+x_{23}w_{23}+x_{31}w_{31}+x_{32}w_{32}+x_{33}w_{33} \\
y_{12}=x_{12}w_{11}+x_{13}w_{12}+x_{14}w_{13}+x_{22}w_{21}+x_{23}&w_{22}+x_{24}w_{23}+x_{32}w_{31}+x_{33}w_{32}+x_{34}w_{33} \\
&\vdots
\end{aligned}
$$


#### 池化Pooling

池化一般用于降采样过程，在很多前沿的网络结构中以经不再使用该处理过程，转而直接使用纯卷积网络

![](.\image\池化.png)

##### 均值池化

取区域内元素的平均值
$$
\begin{aligned}
y_{11}=\frac{x_{11}+x_{12}+x_{21}+x_{22}}{4} \\
y_{12}=\frac{x_{13}+x_{14}+x_{23}+x_{24}}{4} \\
y_{21}=\frac{x_{31}+x_{32}+x_{41}+x_{42}}{4} \\
y_{22}=\frac{x_{33}+x_{34}+x_{43}+x_{44}}{4}
\end{aligned}
$$


##### 最大值池化

取区域内元素的最大值
$$
\begin{aligned}
y_{11}=max(x_{11},x_{12},x_{21},x_{22}) \\
y_{12}=max(x_{13},x_{14},x_{23},x_{24}) \\
y_{21}=max(x_{31},x_{32},x_{41},x_{42}) \\
y_{22}=max(x_{33},x_{34},x_{43},x_{44})
\end{aligned}
$$

#### 卷积计算

卷积神经网络最主要的计算消耗就是卷积计算

##### im2col

图像的卷积计算有很多种算法，目前各个开源框架常用的都是im2col+gemm的方式

该方法的做法，就是将卷积过程转化为矩阵乘法，其好处在于可以通过优化矩阵乘算法，优化计算过程，并且有利于CUDA等并行计算

设有图像A，它的一个通道数据如下：

![](C:\Users\yuzhu\Desktop\image\im2col.png)

卷积核W，展开为列向量如下：

![](C:\Users\yuzhu\Desktop\image\kernel.png)

我们将每次卷积计算时，卷积核覆盖的元素分别列出如下：

![](C:\Users\yuzhu\Desktop\image\展开.png)

将这些展开的元素行向量组合为矩阵如下：

![](C:\Users\yuzhu\Desktop\image\imcol.png)

则卷积过程可表示为如下矩阵乘：

![](C:\Users\yuzhu\Desktop\image\乘.png)

则每一次卷积运算可表示如下：
$$
\begin{aligned}
&z_{l}=w_{l}a_{l-1} \\
&a_{l}=\sigma(z_{l})=\sigma(w_{l}a_{l-1})
\end{aligned}
$$


##### 一些细节

**图像的多通道**

卷积层的图像都是三维张量n\*m\*z，对于多通道的图像使用一个卷积核卷积，每一个通道都会生成一份结果，我们需要将它们累加起来

设有一个通道为2的图像，我们的卷积过程如下

通道1：

![](C:\Users\yuzhu\Desktop\image\通道1.png)

通道2：

![](C:\Users\yuzhu\Desktop\image\通道2.png)

最终卷积结果应该为：

![](C:\Users\yuzhu\Desktop\image\加.png)

在代码实现时，往往先将不同通道的元素相加再做矩阵乘



**多卷积核**

一个卷积核将会得到一个结果，则n个卷积核得到n个结果，组成卷积结果图像的n个通道

layer     filters    size              input                output
    0 conv      4  3 x 3 / 1    48 x  48 x   3   ->    48 x  48 x   4  0.000 BFLOPs
    1 max          2 x 2 / 2    48 x  48 x   4   ->    24 x  24 x   4
    2 conv      8  3 x 3 / 2    24 x  24 x   4   ->    12 x  12 x   8  0.000 BFLOPs
    3 max          2 x 2 / 2    12 x  12 x   8   ->     6 x   6 x   8
    4 conv     16  1 x 1 / 1     6 x   6 x   8   ->     6 x   6 x  16  0.000 BFLOPs
    5 softmax                                         576
